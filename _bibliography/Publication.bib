---
---
@article{spa_vl,
author={Yongting Zhang, Lu Chen, Guodong Zheng, Yifeng Gao, Rui Zheng, Jinlan Fu, Zhenfei Yin, Senjie Jin, Yu Qiao, Xuanjing Huang, Feng Zhao, Tao Gui2,, Jing Shao},
journal = {coRR},
title={SPA-VL: A Comprehensive Safety Preference Alignment Dataset for Vision Language Model},
volume={},
year={2024},
website={https://sqrtizhang.github.io/SPA-VL/},
preview={/assets/img/SPA-VL.png}
}

@article{an_1st_2022,
 author = {An, Dong and Wang, Zun and Li, Yangguang and Wang, Yi and Hong, Yicong and Huang, Yan and Wang, Liang and Shao, Jing},
 journal = {CoRR},
 title = {1st Place Solutions for RxR-Habitat Vision-and-Language Navigation Competition (CVPR 2022)},
 volume = {abs/2206.11610},
 year = {2022}
}


@article{chen_ergo_2022,
 author = {Chen, Meiqi and Cao, Yixin and Deng, Kunquan and Li, Mukai and Wang, Kun and Shao, Jing and Zhang, Yan},
 journal = {CoRR},
 shorttitle = {ERGO},
 title = {ERGO: Event Relational Graph Transformer for Document-level Event Causality Identification},
 volume = {abs/2204.07434},
 year = {2022}
}


@article{cui_democratizing_2022,
 author = {Cui, Yufeng and Zhao, Lichen and Liang, Feng and Li, Yangguang and Shao, Jing},
 journal = {CoRR},
 shorttitle = {Democratizing Contrastive Language-Image Pre-training},
 title = {Democratizing Contrastive Language-Image Pre-training: A CLIP Benchmark of Data, Model, and Supervision},
 volume = {abs/2203.05796},
 year = {2022}
}


@article{he_x-learner_2022,
 author = {He, Yinan and Huang, Gengshi and Chen, Siyu and Teng, Jianing and Kun, Wang and Yin, Zhenfei and Sheng, Lu and Liu, Ziwei and Qiao, Yu and Shao, Jing},
 journal = {CoRR},
 shorttitle = {X-Learner},
 title = {X-Learner: Learning Cross Sources and Tasks for Universal Visual Representation},
 volume = {abs/2203.08764},
 year = {2022}
}


@inproceedings{li_supervision_2022,
 author = {Li, Yangguang and Liang, Feng and Zhao, Lichen and Cui, Yufeng and Ouyang, Wanli and Shao, Jing and Yu, Fengwei and Yan, Junjie},
 booktitle = {International Conference on Learning Representations},
 month = {March},
 shorttitle = {Supervision Exists Everywhere},
 title = {Supervision Exists Everywhere: A Data Efficient Contrastive Language-Image Pre-training Paradigm},
 year = {2022}
}


@inproceedings{ma-etal-2022-mmekg,
    title = "{MMEKG}: Multi-modal Event Knowledge Graph towards Universal Representation across Modalities",
    author = "Ma, Yubo  and
      Wang, Zehao  and
      Li, Mukai  and
      Cao, Yixin  and
      Chen, Meiqi  and
      Li, Xinze  and
      Sun, Wenqi  and
      Deng, Kunquan  and
      Wang, Kun  and
      Sun, Aixin  and
      Shao, Jing",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics: System Demonstrations",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-demo.23",
    doi = "10.18653/v1/2022.acl-demo.23",
    pages = "231--239"
}


@inproceedings{ma_prompt_2022,
 address = {Dublin, Ireland},
 author = {Ma, Yubo and Wang, Zehao and Cao, Yixin and Li, Mukai and Chen, Meiqi and Wang, Kun and Shao, Jing},
 booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
 doi = {10.18653/v1/2022.acl-long.466},
 pages = {6759--6774},
 publisher = {Association for Computational Linguistics},
 title = {Prompt for Extraction? PAIE: Prompting Argument Interaction for Event Argument Extraction},
 year = {2022}
}


@article{pan_st-adapter_2022,
 author = {Pan, Junting and Lin, Ziyi and Zhu, Xiatian and Shao, Jing and Li, Hongsheng},
 journal = {CoRR},
 shorttitle = {ST-Adapter},
 title = {ST-Adapter: Parameter-Efficient Image-to-Video Transfer Learning for Action Recognition},
 volume = {abs/2206.13559},
 year = {2022}
}


@article{qiu_few-shot_2022,
 author = {Qiu, Haonan and Chen, Siyu and Gan, Bei and Wang, Kun and Shi, Huafeng and Shao, Jing and Liu, Ziwei},
 journal = {CoRR},
 title = {Few-shot Forgery Detection via Guided Adversarial Interpolation},
 volume = {abs/2204.05905},
 year = {2022}
}


@article{tang_task-balanced_2022,
 author = {Tang, Ruining and Liu, Zhenyu and Li, Yangguang and Song, Yiguo and Liu, Hui and Wang, Qide and Shao, Jing and Duan, Guifang and Tan, Jianrong},
 journal = {CoRR},
 title = {Task-Balanced Distillation for Object Detection},
 volume = {abs/2208.03006},
 year = {2022}
}


@inproceedings{wang_repre_2022,
 author = {Wang, Luya and Liang, Feng and Li, Yangguang and Zhang, Honggang and Ouyang, Wanli and Shao, Jing},
 booktitle = {Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence},
 doi = {10.24963/ijcai.2022/200},
 month = {July},
 pages = {1437--1443},
 title = {RePre: Improving Self-Supervised Vision Transformer with Reconstructive Pre-training},
 volume = {2},
 year = {2022}
}


@article{wang_sncse_2022,
 author = {Wang, Hao and Li, Yangguang and Huang, Zhen and Dou, Yong and Kong, Lingpeng and Shao, Jing},
 journal = {CoRR},
 shorttitle = {SNCSE},
 title = {SNCSE: Contrastive Learning for Unsupervised Sentence Embedding with Soft Negative Samples},
 volume = {abs/2201.05979},
 year = {2022}
}


@article{zhang_bamboo_2022,
 author = {Zhang, Yuanhan and Sun, Qinghong and Zhou, Yichun and He, Zexin and Yin, Zhenfei and Wang, Kun and Sheng, Lu and Qiao, Yu and Shao, Jing and Liu, Ziwei},
 journal = {CoRR},
 shorttitle = {Bamboo},
 title = {Bamboo: Building Mega-Scale Vision Dataset Continually with Human-Machine Synergy},
 volume = {abs/2203.07845},
 year = {2022}
}


@article{zhang_benchmarking_2022,
 author = {Zhang, Yuanhan and Yin, Zhenfei and Shao, Jing and Liu, Ziwei},
 journal = {CoRR},
 title = {Benchmarking Omni-Vision Representation through the Lens of Visual Realms},
 volume = {abs/2207.07106},
 year = {2022}
}


@article{zhang_robust_2022,
 author = {Zhang, Yuanhan and Wu, Yichao and Yin, Zhenfei and Shao, Jing and Liu, Ziwei},
 journal = {CoRR},
 title = {Robust Face Anti-Spoofing with Dual Probabilistic Modeling},
 volume = {abs/2204.12685},
 year = {2022}
}


---
# Documentation: https://wowchemy.com/docs/managing-content/

title: "ST-Adapter: Parameter-efficient Image-to-Video Transfer Learning"
authors:
- Junting Pan
- Ziyi Lin
- Xiatian Zhu
- Jing Shao
- Hongsheng Li
author_notes:
- Equal Contribution
- Equal Contribution
date: '2022-12-06'
doi: ""

# Schedule page publish date (NOT publication's date).
publishDate: 2023-06-13T15:13:44+08:00

# Publication type.
# Legend: 0 = Uncategorized; 1 = Conference paper; 2 = Journal article;
# 3 = Preprint / Working Paper; 4 = Report; 5 = Book; 6 = Book section;
# 7 = Thesis; 8 = Patent
publication_types: ["1"]

# Publication name and optional abbreviated publication name.
publication: "Advances in Neural Information Processing Systems"
publication_short: "NeurIPS"

abstract: "Capitalizing on large pre-trained models for various downstream tasks of interest have recently emerged with promising performance. Due to the ever-growing model size, the standard full fine-tuning based task adaptation strategy becomes prohibitively costly in terms of model training and storage. This has led to a new research direction in parameter-efficient transfer learning. However, existing attempts typically focus on downstream tasks from the same modality (eg, image understanding) of the pre-trained model. This creates a limit because in some specific modalities,(eg, video understanding) such a strong pre-trained model with sufficient knowledge is less or not available. In this work, we investigate such a novel cross-modality transfer learning setting, namely parameter-efficient image-to-video transfer learning. To solve this problem, we propose a new Spatio-Temporal Adapter (ST-Adapter) for parameter-efficient fine-tuning per video task. With a built-in spatio-temporal reasoning capability in a compact design, ST-Adapter enables a pre-trained image model without temporal knowledge to reason about dynamic video content at a small~ 8% per-task parameter cost, requiring approximately 20 times fewer updated parameters compared to previous work. Extensive experiments on video action recognition tasks show that our ST-Adapter can match or even outperform the strong full fine-tuning strategy and state-of-the-art video models, whilst enjoying the advantage of parameter efficiency."

# Summary. An optional shortened abstract.
summary: ""

tags: []
categories: []
featured: false

# Custom links (optional).
#   Uncomment and edit lines below to show custom links.
# links:
# - name: Follow
#   url: https://twitter.com
#   icon_pack: fab
#   icon: twitter

url_pdf: 'https://proceedings.neurips.cc/paper_files/paper/2022/file/a92e9165b22d4456fc6d87236e04c266-Paper-Conference.pdf'
url_code: 'https://github.com/linziyi96/st-adapter'
url_dataset:
url_poster:
url_project:
url_slides:
url_source:
url_video:

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder. 
# Focal points: Smart, Center, TopLeft, Top, TopRight, Left, Right, BottomLeft, Bottom, BottomRight.
image:
  caption: ""
  focal_point: ""
  preview_only: false

# Associated Projects (optional).
#   Associate this publication with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `internal-project` references `content/project/internal-project/index.md`.
#   Otherwise, set `projects: []`.
projects: []

# Slides (optional).
#   Associate this publication with Markdown slides.
#   Simply enter your slide deck's filename without extension.
#   E.g. `slides: "example"` references `content/slides/example/index.md`.
#   Otherwise, set `slides: ""`.
slides: ""
---


@inproceedings{pan_actor-context-actor_2021,
 address = {Nashville, TN, USA},
 author = {Pan, Junting and Chen, Siyu and Shou, Mike Zheng and Liu, Yu and Shao, Jing and Li, Hongsheng},
 booktitle = {2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
 doi = {10.1109/CVPR46437.2021.00053},
 month = {June},
 pages = {464--474},
 publisher = {IEEE},
 title = {Actor-Context-Actor Relation Network for Spatio-Temporal Action Localization},
 year = {2021}
}


@article{he_forgerynet_2021-1,
 author = {He, Yinan and Sheng, Lu and Shao, Jing and Liu, Ziwei and Zou, Zhaofan and Guo, Zhizhi and Jiang, Shan and Sun, Curitis and Zhang, Guosheng and Wang, Keyao and Yue, Haixiao and Hong, Zhibin and Wang, Wanguo and Li, Zhenyu and Wang, Qi and Wang, Zhenli and Xu, Ronghao and Zhang, Mingwen and Wang, Zhiheng and Huang, Zhenhang and Zhang, Tianming and Zhao, Ningning},
 journal = {CoRR},
 shorttitle = {ForgeryNet - Face Forgery Analysis Challenge 2021},
 title = {ForgeryNet - Face Forgery Analysis Challenge 2021: Methods and Results},
 volume = {abs/2112.08325},
 year = {2021}
}


@inproceedings{he_forgerynet_2021,
 author = {He, Yinan and Gan, Bei and Chen, Siyu and Zhou, Yichun and Yin, Guojun and Song, Luchuan and Sheng, Lu and Shao, Jing and Liu, Ziwei},
 booktitle = {2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
 doi = {10.1109/CVPR46437.2021.00434},
 month = {June},
 pages = {4358--4367},
 publisher = {IEEE},
 shorttitle = {ForgeryNet},
 title = {ForgeryNet: A Versatile Benchmark for Comprehensive Forgery Analysis},
 year = {2021}
}


@article{ma_simple_2021,
 author = {Ma, Teli and Geng, Shijie and Wang, Mengmeng and Shao, Jing and Lu, Jiasen and Li, Hongsheng and Gao, Peng and Qiao, Yu},
 journal = {CoRR},
 title = {A Simple Long-Tailed Recognition Baseline via Vision-Language Model},
 volume = {abs/2111.14745},
 year = {2021}
}


@article{yang_few-shot_2021,
 author = {Yang, Bowen and Zhang, Jing and Yin, Zhenfei and Shao, Jing},
 journal = {CoRR},
 title = {Few-Shot Domain Expansion for Face Anti-Spoofing},
 volume = {abs/2106.14162},
 year = {2021}
}


@article{zhong_blockqnn_2021,
 abstract = {Convolutional neural networks have gained a remarkable success in computer vision. However, most popular network architectures are hand-crafted and usually require expertise and elaborate design. In this paper, we provide a block-wise network generation pipeline called BlockQNN which automatically builds high-performance networks using the Q-Learning paradigm with epsilon-greedy exploration strategy. The optimal network block is constructed by the learning agent which is trained to choose component layers sequentially. We stack the block to construct the whole auto-generated network. To accelerate the generation process, we also propose a distributed asynchronous framework and an early stop strategy. The block-wise generation brings unique advantages: (1) it yields state-of-the-art results in comparison to the hand-crafted networks on image classification, particularly, the best network generated by BlockQNN achieves 2.35 percent top-1 error rate on CIFAR-10. (2) it offers tremendous reduction of the search space in designing networks, spending only 3 days with 32 GPUs. A faster version can yield a comparable result with only 1 GPU in 20 hours. (3) it has strong generalizability in that the network built on CIFAR also performs well on the larger-scale dataset. The best network achieves very competitive accuracy of 82.0 percent top-1 and 96.0 percent top-5 on ImageNet.},
 author = {Zhong, Zhao and Yang, Zichen and Deng, Boyang and Yan, Junjie and Wu, Wei and Shao, Jing and Liu, Cheng-Lin},
 doi = {10.1109/TPAMI.2020.2969193},
 file = {IEEE Xplore Abstract Record:/Users/lucasjing/Zotero/storage/RTI8PPA5/8966988.html:text/html;Submitted Version:/Users/lucasjing/Zotero/storage/XGTIUGPG/Zhong et al. - 2021 - BlockQNN Efficient Block-Wise Neural Network Arch.pdf:application/pdf},
 issn = {1939-3539},
 journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
 keywords = {Acceleration, AutoML, Computer architecture, Convolutional neural network, Graphics processing units, Indexes, Network architecture, neural architecture search, Neural networks, Q-learning, reinforcement learning, Task analysis},
 month = {July},
 note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
 number = {7},
 pages = {2314--2328},
 shorttitle = {BlockQNN},
 title = {BlockQNN: Efficient Block-Wise Neural Network Architecture Generation},
 volume = {43},
 year = {2021}
}


@article{chen_1st_2020,
 author = {Chen, Siyu and Pan, Junting and Song, Guanglu and Zhang, Manyuan and Shao, Hao and Lin, Ziyi and Shao, Jing and Li, Hongsheng and Liu, Yu},
 journal = {CoRR},
 title = {1st place solution for AVA-Kinetics Crossover in AcitivityNet Challenge 2020},
 volume = {abs/2006.09116},
 year = {2020}
}



@article{sheng_high-quality_2020,
 abstract = {This paper proposes a novel unsupervised video generation that is conditioned on a single structural annotation map, which in contrast to prior conditioned video generation approaches, provides a good balance between motion flexibility and visual quality in the generation process. Different from end-to-end approaches that model the scene appearance and dynamics in a single shot, we try to decompose this difficult task into two easier sub-tasks in a divide-and-conquer fashion, thus achieving remarkable results overall. The first sub-task is an image-to-image (I2I) translation task that synthesizes high-quality starting frame from the input structural annotation map. The second image-to-video (I2V) generation task applies the synthesized starting frame and the associated structural annotation map to animate the scene dynamics for the generation of a photorealistic and temporally coherent video. We employ a cycle-consistent flow-based conditioned variational autoencoder to capture the long-term motion distributions, by which the learned bi-directional flows ensure the physical reliability of the predicted motions and provide explicit occlusion handling in a principled manner. Integrating structural annotations into the flow prediction also improves the structural awareness in the I2V generation process. Quantitative and qualitative evaluations over the autonomous driving and human action datasets demonstrate the effectiveness of the proposed approach over the state-of-the-art methods. The code has been released: https://github.com/junting/seg2vid.},
 author = {Sheng, Lu and Pan, Junting and Guo, Jiaming and Shao, Jing and Loy, Chen Change},
 doi = {10.1007/s11263-020-01334-x},
 issn = {1573-1405},
 journal = {International Journal of Computer Vision},
 keywords = {Conditioned generative model, Image and video synthesis, Motion prediction and estimatiovn, Unsupervised learning},
 language = {en},
 month = {November},
 number = {10},
 pages = {2552--2569},
 title = {High-Quality Video Generation from Static Structural Annotations},
 url = {https://doi.org/10.1007/s11263-020-01334-x},
 urldate = {2022-08-22},
 volume = {128},
 year = {2020}
}


@inproceedings{liu_morphing_2020,
 author = {Liu, Minghua and Sheng, Lu and Yang, Sheng and Shao, Jing and Hu, Shi-Min},
 booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
 doi = {10.1609/aaai.v34i07.6827},
 month = {April},
 note = {Number: 07},
 pages = {11596--11603},
 title = {Morphing and Sampling Network for Dense Point Cloud Completion},
 volume = {34},
 year = {2020}
}


@incollection{vedaldi_celeba-spoof_2020,
 author = {Zhang, Yuanhan and Yin, ZhenFei and Li, Yidong and Yin, Guojun and Yan, Junjie and Shao, Jing and Liu, Ziwei},
 booktitle = {Computer Vision – ECCV 2020},
 doi = {10.1007/978-3-030-58610-2_5},
 pages = {70--85},
 publisher = {Springer},
 title = {CelebA-Spoof: Large-Scale Face Anti-spoofing Dataset with Rich Annotations},
 volume = {12357},
 year = {2020}
}


@incollection{vedaldi_learning_2020,
 author = {Yuan, Kun and Li, Quanquan and Shao, Jing and Yan, Junjie},
 booktitle = {Computer Vision – ECCV 2020},
 doi = {10.1007/978-3-030-58589-1_44},
 pages = {737--753},
 publisher = {Springer International Publishing},
 title = {Learning Connectivity of Neural Networks from a Topological Perspective},
 volume = {12366},
 year = {2020}
}


@incollection{vedaldi_powering_2020,
 author = {Guo, Ronghao and Lin, Chen and Li, Chuming and Tian, Keyu and Sun, Ming and Sheng, Lu and Yan, Junjie},
 booktitle = {Computer Vision – ECCV 2020},
 doi = {10.1007/978-3-030-58568-6_37},
 pages = {625--641},
 publisher = {Springer},
 title = {Powering One-Shot Topological NAS with Stabilized Share-Parameter Proxy},
 volume = {12359},
 year = {2020}
}


@incollection{vedaldi_thinking_2020,
 author = {Qian, Yuyang and Yin, Guojun and Sheng, Lu and Chen, Zixuan and Shao, Jing},
 booktitle = {Computer Vision – ECCV 2020},
 doi = {10.1007/978-3-030-58610-2_6},
 pages = {86--103},
 publisher = {Springer},
 title = {Thinking in Frequency: Face Forgery Detection by Mining Frequency-Aware Clues},
 volume = {12357},
 year = {2020}
}


@article{wang_pv-nas_2020,
 author = {Wang, Zihao and Lin, Chen and Sheng, Lu and Yan, Junjie and Shao, Jing},
 journal = {CoRR},
 shorttitle = {PV-NAS},
 title = {PV-NAS: Practical Neural Architecture Search for Video Recognition},
 volume = {abs/2011.00826},
 year = {2020}
}


@article{sheng_unsupervised_2019,
 author = {Sheng, Lu and Pan, Junting and Guo, Jiaming and Shao, Jing and Wang, Xiaogang and Loy, Chen Change},
 journal = {CoRR},
 title = {Unsupervised Bi-directional Flow-based Video Generation from one Snapshot},
 volume = {abs/1903.00913},
 year = {2019}
}
@article{wang_pv-nas_2020,
 author = {Wang, Zihao and Lin, Chen and Sheng, Lu and Yan, Junjie and Shao, Jing},
 journal = {CoRR},
 shorttitle = {PV-NAS},
 title = {PV-NAS: Practical Neural Architecture Search for Video Recognition},
 volume = {abs/2011.00826},
 year = {2020}
}


@inproceedings{liu_improving_2019,
 address = {Long Beach, CA, USA},
 author = {Liu, Xihui and Wang, Zihao and Shao, Jing and Wang, Xiaogang and Li, Hongsheng},
 booktitle = {2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
 doi = {10.1109/CVPR.2019.00205},
 month = {June},
 pages = {1950--1959},
 publisher = {IEEE},
 title = {Improving Referring Expression Grounding With Cross-Modal Attention-Guided Erasing},
 year = {2019}
}


@inproceedings{liu_learning_2019,
 author = {Liu, Xihui and Yin, Guojun and Shao, Jing and Wang, Xiaogang and Li, hongsheng},
 booktitle = {Advances in Neural Information Processing Systems},
 publisher = {Curran Associates, Inc.},
 title = {Learning to Predict Layout-to-image Conditional Convolutions for Semantic Image Synthesis},
 volume = {32},
 year = {2019}
}


@inproceedings{pan_video_2019,
 abstract = {This paper proposes the novel task of video generation conditioned on a SINGLE semantic label map, which provides a good balance between ﬂexibility and quality in the generation process. Different from typical end-to-end approaches, which model both scene content and dynamics in a single step, we propose to decompose this difﬁcult task into two sub-problems. As current image generation methods do better than video generation in terms of detail, we synthesize high quality content by only generating the ﬁrst frame. Then we animate the scene based on its semantic meaning to obtain temporally coherent video, giving us excellent results overall. We employ a cVAE for predicting optical ﬂow as a beneﬁcial intermediate step to generate a video sequence conditioned on the initial single frame. A semantic label map is integrated into the ﬂow prediction module to achieve major improvements in the image-to-video generation process. Extensive experiments on the Cityscapes dataset show that our method outperforms all competing methods. The source code will be released on https://github.com/junting/seg2vid.},
 address = {Long Beach, CA, USA},
 author = {Pan, Junting and Wang, Chengyu and Jia, Xu and Shao, Jing and Sheng, Lu and Yan, Junjie and Wang, Xiaogang},
 booktitle = {2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
 doi = {10.1109/CVPR.2019.00385},
 file = {Pan et al. - 2019 - Video Generation From Single Semantic Label Map.pdf:/Users/lucasjing/Zotero/storage/FDP7NVUN/Pan et al. - 2019 - Video Generation From Single Semantic Label Map.pdf:application/pdf},
 isbn = {978-1-72813-293-8},
 language = {en},
 month = {June},
 pages = {3728--3737},
 publisher = {IEEE},
 title = {Video Generation From Single Semantic Label Map},
 url = {https://ieeexplore.ieee.org/document/8953551/},
 urldate = {2022-08-22},
 year = {2019}
}


@inproceedings{wang_camp_2019,
 address = {Seoul, Korea (South)},
 author = {Wang, Zihao and Liu, Xihui and Li, Hongsheng and Sheng, Lu and Yan, Junjie and Wang, Xiaogang and Shao, Jing},
 booktitle = {2019 IEEE/CVF International Conference on Computer Vision (ICCV)},
 doi = {10.1109/ICCV.2019.00586},
 month = {October},
 pages = {5763--5772},
 publisher = {IEEE},
 title = {CAMP: Cross-Modal Adaptive Message Passing for Text-Image Retrieval},
 year = {2019}
}


@inproceedings{yin_context_2019,
 abstract = {Dense captioning aims at simultaneously localizing semantic regions and describing these regions-of-interest (ROIs) with short phrases or sentences in natural language. Previous studies have shown remarkable progresses, but they are often vulnerable to the aperture problem that a caption generated by the features inside one ROI lacks contextual coherence with its surrounding context in the input image. In this work, we investigate contextual reasoning based on multi-scale message propagations from the neighboring contents to the target ROIs. To this end, we design a novel end-to-end context and attribute grounded dense captioning framework consisting of 1) a contextual visual mining module and 2) a multi-level attribute grounded description generation module. Knowing that captions often co-occur with the linguistic attributes (such as who, what and where), we also incorporate an auxiliary supervision from hierarchical linguistic attributes to augment the distinctiveness of the learned captions. Extensive experiments and ablation studies on Visual Genome dataset demonstrate the superiority of the proposed model in comparison to the state-of-the-art methods.},
 address = {Long Beach, CA, USA},
 author = {Yin, Guojun and Sheng, Lu and Liu, Bin and Yu, Nenghai and Wang, Xiaogang and Shao, Jing},
 booktitle = {2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
 doi = {10.1109/CVPR.2019.00640},
 file = {Yin et al. - 2019 - Context and Attribute Grounded Dense Captioning.pdf:/Users/lucasjing/Zotero/storage/GKMA6G32/Yin et al. - 2019 - Context and Attribute Grounded Dense Captioning.pdf:application/pdf},
 isbn = {978-1-72813-293-8},
 language = {en},
 month = {June},
 pages = {6234--6243},
 publisher = {IEEE},
 title = {Context and Attribute Grounded Dense Captioning},
 url = {https://ieeexplore.ieee.org/document/8954079/},
 urldate = {2022-08-22},
 year = {2019}
}


@inproceedings{yin_semantics_2019,
 author = {Yin, Guojun and Liu, Bin and Sheng, Lu and Yu, Nenghai and Wang, Xiaogang and Shao, Jing},
 booktitle = {2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
 doi = {10.1109/CVPR.2019.00243},
 month = {June},
 pages = {2322--2331},
 title = {Semantics Disentangling for Text-To-Image Generation},
 year = {2019}
}


@inproceedings{liu_multi-label_2018,
 author = {Liu, Yongcheng and Sheng, Lu and Shao, Jing and Yan, Junjie and Xiang, Shiming and Pan, Chunhong},
 booktitle = {Proceedings of the 26th ACM international conference on Multimedia},
 doi = {10.1145/3240508.3240567},
 keywords = {knowledge distillation, multi-label image classification, weakly-supervised detection},
 pages = {700--708},
 publisher = {Association for Computing Machinery},
 series = {MM '18},
 title = {Multi-Label Image Classification via Knowledge Distillation from Weakly-Supervised Detection},
 year = {2018}
}


@inproceedings{liu_localization_2018,
 author = {Liu, Pengze and Liu, Xihui and Yan, Junjie and Shao, Jing},
 booktitle = {British Machine Vision Conference 2018, BMVC 2018},
 month = {September},
 publisher = {BMVA Press},
 title = {Localization Guided Learning for Pedestrian Attribute Recognition},
 year = {2018}
}


@inproceedings{liu_exploring_2018,
 address = {Salt Lake City, UT, USA},
 author = {Liu, Yu and Wei, Fangyin and Shao, Jing and Sheng, Lu and Yan, Junjie and Wang, Xiaogang},
 booktitle = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
 doi = {10.1109/CVPR.2018.00222},
 month = {June},
 pages = {2080--2089},
 publisher = {IEEE},
 title = {Exploring Disentangled Feature Representation Beyond Face Identification},
 year = {2018}
}


@incollection{ferrari_improving_2018,
 author = {Chen, Dapeng and Li, Hongsheng and Liu, Xihui and Shen, Yantao and Shao, Jing and Yuan, Zejian and Wang, Xiaogang},
 booktitle = {Computer Vision – ECCV 2018},
 doi = {10.1007/978-3-030-01270-0_4},
 pages = {56--73},
 publisher = {Springer},
 title = {Improving Deep Visual Representation for Person Re-identification by Global and Local Image-language Association},
 volume = {11220},
 year = {2018}
}


@incollection{ferrari_show_2018,
 author = {Liu, Xihui and Li, Hongsheng and Shao, Jing and Chen, Dapeng and Wang, Xiaogang},
 booktitle = {Computer Vision – ECCV 2018},
 doi = {10.1007/978-3-030-01267-0_21},
 pages = {353--369},
 publisher = {Springer},
 title = {Show, Tell and Discriminate: Image Captioning by Self-retrieval with Partially Labeled Data},
 volume = {11219},
 year = {2018}
}


@incollection{ferrari_transductive_2018,
 author = {Liu, Yu and Song, Guanglu and Shao, Jing and Jin, Xiao and Wang, Xiaogang},
 booktitle = {Computer Vision – ECCV 2018},
 doi = {10.1007/978-3-030-01228-1_5},
 pages = {72--89},
 publisher = {Springer},
 title = {Transductive Centroid Projection for Semi-supervised Large-Scale Recognition},
 volume = {11209},
 year = {2018}
}


@incollection{ferrari_zoom-net_2018,
 author = {Yin, Guojun and Sheng, Lu and Liu, Bin and Yu, Nenghai and Wang, Xiaogang and Shao, Jing and Loy, Chen Change},
 booktitle = {Computer Vision – ECCV 2018},
 doi = {10.1007/978-3-030-01219-9_20},
 pages = {330--347},
 publisher = {Springer},
 title = {Zoom-Net: Mining Deep Feature Interactions for Visual Relationship Recognition},
 volume = {11207},
 year = {2018}
}


@inproceedings{sheng_avatar-net_2018,
 address = {Salt Lake City, UT, USA},
 author = {Sheng, Lu and Lin, Ziyi and Shao, Jing and Wang, Xiaogang},
 booktitle = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
 doi = {10.1109/CVPR.2018.00860},
 month = {June},
 pages = {8242--8250},
 publisher = {IEEE},
 title = {Avatar-Net: Multi-scale Zero-Shot Style Transfer by Feature Decoration},
 year = {2018}
}


@inproceedings{zhong_practical_2018,
 address = {Salt Lake City, UT},
 author = {Zhong, Zhao and Yan, Junjie and Wu, Wei and Shao, Jing and Liu, Cheng-Lin},
 booktitle = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
 doi = {10.1109/CVPR.2018.00257},
 month = {June},
 pages = {2423--2432},
 publisher = {IEEE},
 title = {Practical Block-Wise Neural Network Architecture Generation},
 year = {2018}
}


@inproceedings{wang_orientation_2017,
 author = {Wang, Zhongdao and Tang, Luming and Liu, Xihui and Yao, Zhuliang and Yi, Shuai and Shao, Jing and Yan, Junjie and Wang, Shengjin and Li, Hongsheng and Wang, Xiaogang},
 booktitle = {2017 IEEE International Conference on Computer Vision (ICCV)},
 doi = {10.1109/ICCV.2017.49},
 pages = {379--387},
 title = {Orientation Invariant Feature Embedding and Spatial Temporal Regularization for Vehicle Re-identification},
 year = {2017}
}


@inproceedings{liu_hydraplus-net_2017,
 address = {Venice},
 author = {Liu, Xihui and Zhao, Haiyu and Tian, Maoqing and Sheng, Lu and Shao, Jing and Yi, Shuai and Yan, Junjie and Wang, Xiaogang},
 booktitle = {2017 IEEE International Conference on Computer Vision (ICCV)},
 doi = {10.1109/ICCV.2017.46},
 month = {October},
 pages = {350--359},
 publisher = {IEEE},
 shorttitle = {HydraPlus-Net},
 title = {HydraPlus-Net: Attentive Deep Features for Pedestrian Analysis},
 year = {2017}
}


@article{shao_crowded_2017,
 author = {Shao, Jing and Loy, Chen Change and Kang, Kai and Wang, Xiaogang},
 doi = {10.1109/TCSVT.2016.2593647},
 journal = {IEEE Transactions on Circuits and Systems for Video Technology},
 month = {March},
 number = {3},
 pages = {613--623},
 title = {Crowded Scene Understanding by Deeply Learned Volumetric Slices},
 volume = {27},
 year = {2017}
}


@article{shao_learning_2017,
 author = {Shao, Jing and Loy, Chen Change and Wang, Xiaogang},
 doi = {10.1109/TCSVT.2016.2539878},
 journal = {IEEE Transactions on Circuits and Systems for Video Technology},
 month = {June},
 number = {6},
 pages = {1290--1303},
 title = {Learning Scene-Independent Group Descriptors for Crowd Understanding},
 volume = {27},
 year = {2017}
}


@inproceedings{zhao_spindle_2017,
 address = {Honolulu, HI, USA},
 author = {Zhao, Haiyu and Tian, Maoqing and Sun, Shuyang and Shao, Jing and Yan, Junjie and Yi, Shuai and Wang, Xiaogang and Tang, Xiaoou},
 booktitle = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
 doi = {10.1109/CVPR.2017.103},
 month = {July},
 pages = {907--915},
 publisher = {IEEE},
 title = {Spindle Net: Person Re-identification with Human Body Region Guided Feature Decomposition and Fusion},
 year = {2017}
}


@inproceedings{shao_slicing_2016,
 abstract = {Learning and capturing both appearance and dynamic representations are pivotal for crowd video understanding. Convolutional Neural Networks (CNNs) have shown its remarkable potential in learning appearance representations from images. However, the learning of dynamic representation, and how it can be effectively combined with appearance features for video analysis, remains an open problem. In this study, we propose a novel spatio-temporal CNN, named Slicing CNN (S-CNN), based on the decomposition of 3D feature maps into 2D spatio-and 2D temporal-slices representations. The decomposition brings unique advantages: (1) the model is capable of capturing dynamics of different semantic units such as groups and objects, (2) it learns separated appearance and dynamic representations while keeping proper interactions between them, and (3) it exploits the selectiveness of spatial filters to discard irrelevant background clutter for crowd understanding. We demonstrate the effectiveness of the proposed S-CNN model on the WWW crowd video dataset for attribute recognition and observe significant performance improvements to the state-of-the-art methods (62.55% from 51.84% [21]).},
 author = {Shao, Jing and Loy, Chen Change and Kang, Kai and Wang, Xiaogang},
 booktitle = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
 doi = {10.1109/CVPR.2016.606},
 file = {IEEE Xplore Abstract Record:/Users/lucasjing/Zotero/storage/DKIDBWAV/7780975.html:text/html},
 keywords = {Dynamics, Feature extraction, Ice, Semantics, Three-dimensional displays, Two dimensional displays, Visualization},
 month = {June},
 note = {ISSN: 1063-6919},
 pages = {5620--5628},
 title = {Slicing Convolutional Neural Network for Crowd Video Understanding},
 year = {2016}
}


@inproceedings{shao_deeply_2015,
 author = {Shao, Jing and Kang, Kai and Loy, Chen Change and Wang, Xiaogang},
 booktitle = {2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
 doi = {10.1109/CVPR.2015.7299097},
 month = {June},
 pages = {4657--4666},
 title = {Deeply learned attributes for crowded scene understanding},
 year = {2015}
}


@inproceedings{shao_scene-independent_2014,
 author = {Shao, Jing and Loy, Chen Change and Wang, Xiaogang},
 booktitle = {2014 IEEE Conference on Computer Vision and Pattern Recognition},
 doi = {10.1109/CVPR.2014.285},
 month = {June},
 pages = {2227--2234},
 title = {Scene-Independent Group Profiling in Crowd},
 year = {2014}
}
